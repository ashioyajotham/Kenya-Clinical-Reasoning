{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the project root to path\n",
    "project_root = os.path.dirname(os.path.abspath('.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "!pip install -q pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import benchmark components\n",
    "from Benchmarking import (\n",
    "    # Runner\n",
    "    BenchmarkRunner, BenchmarkConfig, ModelConfig,\n",
    "    run_kenya_clinical_benchmark,\n",
    "    \n",
    "    # Metrics\n",
    "    ClinicalMetrics, BootstrapEvaluator,\n",
    "    \n",
    "    # Datasets\n",
    "    TaskType, BenchmarkSample, BenchmarkDataset, KenyaClinicalDataLoader,\n",
    "    \n",
    "    # Retrieval\n",
    "    ClinicalRetriever, ClinicalKnowledgeBase,\n",
    "    \n",
    "    # Tasks\n",
    "    ClinicalReasoningTask, DiagnosticTask, MultiChoiceTask, RetrievalQATask\n",
    ")\n",
    "\n",
    "print(\"âœ… All benchmark components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb99a37",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Test Metrics Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "metrics = ClinicalMetrics()\n",
    "print(\"ClinicalMetrics initialized\")\n",
    "print(f\"  Sentence Transformers available: {metrics.st_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baeb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ROUGE computation\n",
    "prediction = \"The patient likely has acute coronary syndrome. Recommend ECG and cardiac enzymes.\"\n",
    "reference = \"Possible acute coronary syndrome. DDX includes MI, unstable angina. Immediate ECG, cardiac enzymes recommended.\"\n",
    "\n",
    "rouge_scores = metrics.compute_rouge(prediction, reference)\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50858ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic similarity\n",
    "semantic_sim = metrics.compute_semantic_similarity(prediction, reference)\n",
    "print(f\"\\nSemantic Similarity: {semantic_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DDX accuracy\n",
    "predicted_ddx = [\"Acute coronary syndrome\", \"Myocardial infarction\", \"Unstable angina\", \"GERD\"]\n",
    "true_ddx = [\"Acute coronary syndrome\", \"MI\", \"Pneumonia\"]\n",
    "\n",
    "ddx_metrics = metrics.compute_ddx_accuracy(predicted_ddx, true_ddx)\n",
    "print(\"\\nDDX Accuracy Metrics:\")\n",
    "for metric, score in ddx_metrics.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c17950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clinical completeness\n",
    "response = \"\"\"\n",
    "ASSESSMENT: The patient presents with classic signs of acute coronary syndrome.\n",
    "DIFFERENTIAL DIAGNOSIS: 1) STEMI, 2) NSTEMI, 3) Unstable angina\n",
    "INVESTIGATIONS: ECG, Troponin, Chest X-ray\n",
    "MANAGEMENT: Aspirin, Heparin, Consider PCI\n",
    "FOLLOW-UP: Cardiology referral within 24 hours\n",
    "\"\"\"\n",
    "\n",
    "completeness = metrics.compute_clinical_completeness(\n",
    "    response, \n",
    "    required_elements=['assessment', 'diagnosis', 'investigation', 'management', 'follow-up']\n",
    ")\n",
    "print(f\"\\nClinical Completeness Score: {completeness:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de90658",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11193ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if data file exists\n",
    "data_path = \"train.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"âœ… Loaded training data: {len(df)} samples\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nSample prompt (truncated):\")\n",
    "    print(df['Prompt'].iloc[0][:500] + \"...\")\n",
    "else:\n",
    "    print(f\"âŒ Data file not found at: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f687083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = KenyaClinicalDataLoader(\n",
    "    train_path=\"train.csv\",\n",
    "    val_split=0.2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… KenyaClinicalDataLoader initialized\")\n",
    "print(f\"  Total samples: {len(loader.train_df)}\")\n",
    "print(f\"  Train samples: {len(loader.train_df) - int(len(loader.train_df) * 0.2)}\")\n",
    "print(f\"  Val samples: {int(len(loader.train_df) * 0.2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Clinical Reasoning dataset\n",
    "clinical_dataset = loader.create_clinical_reasoning_dataset()\n",
    "\n",
    "print(f\"\\nâœ… Clinical Reasoning Dataset Created\")\n",
    "print(f\"  Name: {clinical_dataset.name}\")\n",
    "print(f\"  Task Type: {clinical_dataset.task_type}\")\n",
    "print(f\"  Samples: {len(clinical_dataset.samples)}\")\n",
    "\n",
    "# Show first sample\n",
    "if clinical_dataset.samples:\n",
    "    sample = clinical_dataset.samples[0]\n",
    "    print(f\"\\n  First sample:\")\n",
    "    print(f\"    ID: {sample.id}\")\n",
    "    print(f\"    Input (truncated): {sample.input_text[:200]}...\")\n",
    "    print(f\"    Expected output (truncated): {sample.expected_output[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Diagnostic dataset\n",
    "diagnostic_dataset = loader.create_diagnostic_dataset()\n",
    "\n",
    "print(f\"\\nâœ… Diagnostic Dataset Created\")\n",
    "print(f\"  Samples: {len(diagnostic_dataset.samples)}\")\n",
    "\n",
    "if diagnostic_dataset.samples:\n",
    "    sample = diagnostic_dataset.samples[0]\n",
    "    print(f\"  First sample metadata: {sample.metadata.get('ddx_snomed', 'N/A')[:100] if sample.metadata.get('ddx_snomed') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-Choice dataset\n",
    "mc_dataset = loader.create_multi_choice_dataset()\n",
    "\n",
    "print(f\"\\nâœ… Multi-Choice Dataset Created\")\n",
    "print(f\"  Samples: {len(mc_dataset.samples)}\")\n",
    "\n",
    "if mc_dataset.samples:\n",
    "    sample = mc_dataset.samples[0]\n",
    "    print(f\"  First sample choices: {sample.metadata.get('choices', [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf8b22",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Test Retrieval Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = ClinicalRetriever()\n",
    "\n",
    "# Add sample documents\n",
    "sample_docs = [\n",
    "    {\"id\": \"doc1\", \"text\": \"Management of diabetic ketoacidosis involves IV fluids, insulin therapy, and electrolyte monitoring.\"},\n",
    "    {\"id\": \"doc2\", \"text\": \"Bacterial meningitis requires empirical antibiotics, lumbar puncture for diagnosis, and ICU admission.\"},\n",
    "    {\"id\": \"doc3\", \"text\": \"Malaria treatment in Kenya follows WHO guidelines with artemisinin-based combination therapy.\"},\n",
    "    {\"id\": \"doc4\", \"text\": \"Hypertensive emergency management includes IV labetalol or nitroprusside with continuous monitoring.\"},\n",
    "    {\"id\": \"doc5\", \"text\": \"Pediatric pneumonia assessment includes respiratory rate, chest indrawing, and oxygen saturation.\"},\n",
    "    {\"id\": \"doc6\", \"text\": \"HIV treatment follows Kenya national guidelines with first-line ART including TDF/3TC/DTG.\"},\n",
    "    {\"id\": \"doc7\", \"text\": \"Tuberculosis diagnosis uses GeneXpert MTB/RIF for rapid detection of rifampicin resistance.\"},\n",
    "]\n",
    "\n",
    "retriever.add_documents(sample_docs)\n",
    "print(f\"âœ… Added {len(sample_docs)} documents to retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TF-IDF retrieval\n",
    "query = \"How to manage a diabetic patient with high blood sugar and acidosis?\"\n",
    "\n",
    "results = retriever.retrieve_tfidf(query, top_k=3)\n",
    "\n",
    "print(f\"\\nðŸ” Query: {query}\")\n",
    "print(f\"\\nTF-IDF Retrieval Results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"  {i}. [{r.doc_id}] Score: {r.score:.4f}\")\n",
    "    print(f\"     {r.text[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval evaluation\n",
    "queries = [\n",
    "    \"diabetic ketoacidosis management\",\n",
    "    \"meningitis treatment antibiotics\",\n",
    "    \"malaria treatment guidelines kenya\"\n",
    "]\n",
    "relevant_docs = [[\"doc1\"], [\"doc2\"], [\"doc3\"]]\n",
    "\n",
    "eval_results = retriever.evaluate_retrieval(\n",
    "    queries, relevant_docs, method=\"tfidf\", k_values=[1, 3, 5]\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Retrieval Evaluation:\")\n",
    "print(f\"  MRR: {eval_results['mrr']:.4f}\")\n",
    "for k in [1, 3, 5]:\n",
    "    print(f\"  Recall@{k}: {eval_results.get(f'recall@{k}', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e033e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Test Task Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clinical Reasoning Task\n",
    "cr_task = ClinicalReasoningTask()\n",
    "\n",
    "sample = BenchmarkSample(\n",
    "    id=\"test_cr_001\",\n",
    "    input_text=\"A 45-year-old male farmer from Kisumu presents with fever, headache, and jaundice for 3 days.\",\n",
    "    expected_output=\"DDX includes malaria, hepatitis, leptospirosis. Recommend malaria RDT, LFTs, urinalysis. Start empirical antimalarials pending results.\",\n",
    "    task_type=TaskType.CLINICAL_REASONING,\n",
    "    metadata={'county': 'Kisumu', 'facility': 'County Hospital'}\n",
    ")\n",
    "\n",
    "prediction = \"Patient may have malaria or hepatitis. Recommend malaria test and liver function tests.\"\n",
    "\n",
    "result = cr_task.evaluate_sample(sample, prediction)\n",
    "\n",
    "print(\"\\nðŸ“‹ Clinical Reasoning Task Evaluation:\")\n",
    "print(f\"  Sample ID: {result.sample_id}\")\n",
    "print(f\"  Metrics:\")\n",
    "for metric, value in result.metrics.items():\n",
    "    print(f\"    {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Diagnostic Task\n",
    "diag_task = DiagnosticTask()\n",
    "\n",
    "sample = BenchmarkSample(\n",
    "    id=\"test_diag_001\",\n",
    "    input_text=\"Child with high fever, neck stiffness, and photophobia.\",\n",
    "    expected_output=\"1. Bacterial meningitis\\n2. Viral meningitis\\n3. Subarachnoid hemorrhage\",\n",
    "    task_type=TaskType.DIAGNOSTIC,\n",
    "    metadata={}\n",
    ")\n",
    "\n",
    "prediction = \"1. Meningitis\\n2. Encephalitis\\n3. Brain abscess\\n4. Malaria\"\n",
    "\n",
    "result = diag_task.evaluate_sample(sample, prediction)\n",
    "\n",
    "print(\"\\nðŸ©º Diagnostic Task Evaluation:\")\n",
    "print(f\"  Sample ID: {result.sample_id}\")\n",
    "print(f\"  Predicted DDX: {result.metadata.get('predicted_ddx', [])}\")\n",
    "print(f\"  True DDX: {result.metadata.get('true_ddx', [])}\")\n",
    "print(f\"  Metrics:\")\n",
    "for metric, value in result.metrics.items():\n",
    "    print(f\"    {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Choice Task\n",
    "mc_task = MultiChoiceTask()\n",
    "\n",
    "sample = BenchmarkSample(\n",
    "    id=\"test_mc_001\",\n",
    "    input_text=\"A patient presents with productive cough and fever. What is the most appropriate initial investigation?\",\n",
    "    expected_output=\"B\",\n",
    "    task_type=TaskType.MULTI_CHOICE,\n",
    "    metadata={\n",
    "        'choices': [\"CT scan\", \"Chest X-ray\", \"MRI\", \"PET scan\"],\n",
    "        'correct_answer': 'B'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Format prompt with choices\n",
    "prompt = mc_task.format_prompt(sample)\n",
    "print(\"ðŸ“ Multi-Choice Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "prediction = \"The answer is B. Chest X-ray is the most appropriate initial investigation.\"\n",
    "result = mc_task.evaluate_sample(sample, prediction)\n",
    "\n",
    "print(f\"\\nâœ… Multi-Choice Task Evaluation:\")\n",
    "print(f\"  Predicted: {result.prediction}\")\n",
    "print(f\"  Correct: {result.ground_truth}\")\n",
    "print(f\"  Accuracy: {result.metrics['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee97729",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d38b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark configuration\n",
    "config = BenchmarkConfig(\n",
    "    name=\"kenya_clinical_demo\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"Demo benchmark run\",\n",
    "    tasks=[\"clinical_reasoning\"],  # Start with one task\n",
    "    train_data_path=\"train.csv\",\n",
    "    val_split=0.2,\n",
    "    output_dir=\"benchmark_results\",\n",
    "    save_predictions=True\n",
    ")\n",
    "\n",
    "print(\"âœ… BenchmarkConfig created\")\n",
    "print(f\"  Name: {config.name}\")\n",
    "print(f\"  Tasks: {config.tasks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner\n",
    "runner = BenchmarkRunner(config=config, data_dir=\".\")\n",
    "\n",
    "# Setup (load data, create datasets)\n",
    "runner.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple baseline model (echo model for testing)\n",
    "def baseline_model(prompt: str) -> str:\n",
    "    \"\"\"Simple baseline that extracts key clinical terms.\"\"\"\n",
    "    # This is just for testing - replace with actual model\n",
    "    response = \"\"\"\n",
    "    Based on the clinical presentation:\n",
    "    \n",
    "    ASSESSMENT:\n",
    "    Patient requires urgent evaluation.\n",
    "    \n",
    "    DIFFERENTIAL DIAGNOSIS:\n",
    "    1. Consider infectious etiology\n",
    "    2. Metabolic causes\n",
    "    3. Structural abnormalities\n",
    "    \n",
    "    INVESTIGATIONS:\n",
    "    - Complete blood count\n",
    "    - Basic metabolic panel\n",
    "    - Relevant imaging\n",
    "    \n",
    "    MANAGEMENT:\n",
    "    - Supportive care\n",
    "    - Treat underlying cause\n",
    "    - Monitor closely\n",
    "    \n",
    "    FOLLOW-UP:\n",
    "    - Review in 24-48 hours\n",
    "    - Specialist referral if needed\n",
    "    \"\"\"\n",
    "    return response\n",
    "\n",
    "# Create model config\n",
    "model_config = ModelConfig(\n",
    "    name=\"baseline_model\",\n",
    "    model_fn=baseline_model,\n",
    "    description=\"Simple baseline for testing\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Baseline model configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56485dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Running benchmark evaluation...\")\n",
    "result = runner.evaluate_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b847f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {result.model_name}\")\n",
    "print(f\"Aggregate Score: {result.aggregate_score:.4f}\")\n",
    "print(f\"Timestamp: {result.timestamp}\")\n",
    "\n",
    "for task_name, task_result in result.task_results.items():\n",
    "    print(f\"\\n--- {task_name} ---\")\n",
    "    print(f\"Samples: {task_result.num_samples}\")\n",
    "    print(\"Key Metrics:\")\n",
    "    for metric, value in sorted(task_result.aggregate_metrics.items()):\n",
    "        if 'mean' in metric:\n",
    "            print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "report = runner.generate_report()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3bbd7",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Test with Multiple Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08041cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another model for comparison\n",
    "def improved_model(prompt: str) -> str:\n",
    "    \"\"\"Slightly better baseline.\"\"\"\n",
    "    return \"\"\"\n",
    "    CLINICAL ASSESSMENT:\n",
    "    The patient presents with concerning symptoms requiring systematic evaluation.\n",
    "    \n",
    "    DIFFERENTIAL DIAGNOSIS:\n",
    "    1. Primary consideration based on presentation\n",
    "    2. Secondary differential\n",
    "    3. Must rule out emergent conditions\n",
    "    4. Consider endemic diseases (malaria, TB)\n",
    "    \n",
    "    RECOMMENDED INVESTIGATIONS:\n",
    "    - Complete blood count with differential\n",
    "    - Comprehensive metabolic panel\n",
    "    - Malaria RDT (if febrile)\n",
    "    - Urinalysis\n",
    "    - Chest X-ray if respiratory symptoms\n",
    "    \n",
    "    MANAGEMENT PLAN:\n",
    "    - IV access and fluids if dehydrated\n",
    "    - Empirical antimalarials if indicated\n",
    "    - Symptomatic treatment\n",
    "    - Close monitoring\n",
    "    \n",
    "    DISPOSITION AND FOLLOW-UP:\n",
    "    - Admit if unstable\n",
    "    - Referral to specialist if needed\n",
    "    - Follow up within 48 hours\n",
    "    \"\"\"\n",
    "\n",
    "# Evaluate second model\n",
    "model_config2 = ModelConfig(\n",
    "    name=\"improved_model\",\n",
    "    model_fn=improved_model,\n",
    "    description=\"Improved baseline\"\n",
    ")\n",
    "\n",
    "result2 = runner.evaluate_model(model_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be87a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = runner.compare_models()\n",
    "\n",
    "print(\"\\nðŸ“Š MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRanking (by aggregate score):\")\n",
    "for i, model in enumerate(comparison['ranking'], 1):\n",
    "    score = comparison['aggregate_scores'][model]\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final report with both models\n",
    "final_report = runner.generate_report()\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5464862",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary & Next Steps\n",
    "\n",
    "### âœ… Completed\n",
    "1. **Metrics Module** - ROUGE, semantic similarity, DDX accuracy, clinical completeness\n",
    "2. **Datasets Module** - Clinical reasoning, diagnostic, multi-choice, retrieval QA datasets\n",
    "3. **Retrieval Module** - TF-IDF retrieval with evaluation\n",
    "4. **Tasks Module** - All four task types with evaluators\n",
    "5. **Benchmark Runner** - Full orchestration and reporting\n",
    "\n",
    "### ðŸ”œ Next Steps\n",
    "1. **Add real models** - Integrate FLAN-T5, GPT-4, Gemini evaluations\n",
    "2. **Enable semantic retrieval** - Install sentence-transformers for dense retrieval\n",
    "3. **Expand knowledge base** - Add Kenya clinical guidelines\n",
    "4. **Add more metrics** - BERTScore, BLEU, clinical F1\n",
    "5. **Create leaderboard** - Track model performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ‰ Benchmark framework test completed successfully!\")\n",
    "print(\"\\nFramework Structure:\")\n",
    "print(\"  Benchmarking/\")\n",
    "print(\"  â”œâ”€â”€ __init__.py      - Module exports\")\n",
    "print(\"  â”œâ”€â”€ metrics.py       - Evaluation metrics\")\n",
    "print(\"  â”œâ”€â”€ datasets.py      - Dataset loading\")\n",
    "print(\"  â”œâ”€â”€ retrieval.py     - Knowledge retrieval\")\n",
    "print(\"  â”œâ”€â”€ tasks.py         - Task evaluators\")\n",
    "print(\"  â””â”€â”€ benchmark_runner.py - Main orchestration\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
